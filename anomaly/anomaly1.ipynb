{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matplotlib.rcParams['savefig.dpi'] = 144\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection, Session 1\n",
    "\n",
    "The first two sessions will be focused on anomaly detection in time-series data.\n",
    "\n",
    "**Time series** differ from other sources of data in that they are explicitly ordered.  The usual intent is to use past data to make predictions about the future, so only data from the past may be used to make a prediction.  For simplicity, we will ignore this restriction for most of these sessions, but will discuss it in the context of **online learning** towards the end.\n",
    "\n",
    "**Anomaly detection**, or novelty detection, is attempting to find data that look different from the majority of the data.  It is typically an **unsupervised learning** system.  By this, we mean that the anomalous data is not **labeled**.  We must detect it by learning what the normal data look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CitiBike Ridership Data\n",
    "\n",
    "We will be looking at ridership from the CitiBike bike sharing system.  The data are available [online](https://s3.amazonaws.com/tripdata/index.html).  The zip files should be loaded in to the `anomaly/tripdata/` directory.  The script `download.sh` will do this for you.\n",
    "\n",
    "Let's start by looking at an individual file.  Python's *zipfile* module will allow us to read data from those zip files without manually unzipping each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('tripdata/201307-citibike-tripdata.zip', 'r')\n",
    "zf.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = zf.read(zf.namelist()[0])\n",
    "print '\\n'.join(data.split('\\n')[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *pandas* module provides a `DataFrame` class for powerful manipulation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(zf.open(zf.namelist()[0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get just the day of the start time.  String processing is a bit faster than turning everything into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is count how many time each date occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the index to actual datetime objects for convenience later.  Let's wrap this all up in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_counts(fn):\n",
    "    zf = zipfile.ZipFile(fn, 'r')\n",
    "    df = pd.read_csv(zf.open(zf.namelist()[0]))\n",
    "    counts = df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).value_counts()\n",
    "    if '-' in counts.index[0]:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%Y-%m-%d')\n",
    "    else:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%m/%d/%Y')\n",
    "    return counts.sort_index()\n",
    "\n",
    "load_counts('tripdata/201307-citibike-tripdata.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do this for all of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fns = glob.glob('tripdata/[0-9][0-9][0-9][0-9][0-9][0-9]-citibike-tripdata.zip')\n",
    "counts = pd.concat([load_counts(fn) for fn in sorted(fns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.plot()\n",
    "plt.ylabel('Rides per day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting seasonality\n",
    "\n",
    "**Seasonality** is the tendency of time-series data to have underlying cycles.  We typically wish to remove these variations before undertaking further analysis.\n",
    "\n",
    "The first tool we we look at is the **autocorrelation**.  For two random variables, $X$ and $Y$, the covariance is defined as\n",
    "\n",
    "$$ \\mbox{Cov}[X, Y] = E\\left[(X - E[X])(Y - E[Y])\\right] = E[XY] - E[X]E[Y] \\ .$$\n",
    "\n",
    "If there is no correlation between the random variables, the covariance is 0.  If the two random variables always return the same value,\n",
    "\n",
    "$$ \\mbox{Cov}[X, Y] = \\mbox{Cov}[X, X] = \\mbox{Var}[X] \\ . $$\n",
    "\n",
    "The autocovariance of a time-series signal is just the covariance of the signal with a time-lagged copy.  The autocorrelation normalizes this by the variance of the signal:\n",
    "\n",
    "$$ \\rho(X \\mid \\tau) = \\frac{\\mbox{Cov}[X_t, X_{t+\\tau}]}{\\mbox{Var[X]}} \\ . $$\n",
    "\n",
    "Pandas provides a built-in autocorrelation plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(counts)\n",
    "plt.axvline(365, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yearly cycle is clearly visible.  Zooming in, more detail becomes obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(counts)\n",
    "plt.xlim(0,60)\n",
    "plt.axvline(7, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourier analysis** takes advantage of the fact that any signal can be written as the sum of sinusoids: \n",
    "\n",
    "$$ X_t = \\sum_\\nu A_\\nu \\sin(2\\pi\\nu t) + B_\\nu \\cos(2\\pi\\nu t) \\ . $$\n",
    "\n",
    "The **Fourier transform** is used to read out the coefficients $A_\\nu$ and $B_\\nu$ from the original signal.  If the signal is sampled, this process is known as the **discrete Fourier transform** (DFT).  The standard algorithm for doing this is the **fast Fourier transform** (FFT), which is implemented in the *numpy* module.\n",
    "\n",
    "Quite often, we are not concerned with the Fourier coefficients directly, but with the total power provided at frequency $\\nu$, $A_\\nu^2 + B_\\nu^2$.  This **power spectrum** is easily derived from the Fourier transform.\n",
    "\n",
    "(In practice, Fourier analysis uses complex exponentials instead of sines and cosines.  Instead of real coefficients $A_\\nu$ and $B_\\nu$, the FFT returns a single complex coefficient $C_\\nu$.  The contribution to the power spectrum is $|C_\\nu|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fft_counts = np.fft.fft(counts - counts.mean())\n",
    "yrs = (counts.index[-1] - counts.index[0]).days / 365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.xlabel('Freq (1/yrs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Fourier transform returns information on frequencies up to the sampling frequency (1/day, or 365/year), only the results up to half of that are valid.  This is due to the problem of **aliasing**.  In a sampled signal, you can not distinguish a signal with a frequency above half of the sampling rate, known as the **Nyquist freqency**, from a signal with a frequency below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.linspace(0, 10, 1000)\n",
    "ts = np.arange(0, 11)\n",
    "f = 0.65\n",
    "plt.plot(t, np.sin(2*np.pi * f * t), t, -np.sin(2*np.pi * (1 - f) * t))\n",
    "ml, sl, bl = plt.stem(ts, np.sin(2*np.pi * f * ts))\n",
    "plt.setp(ml, 'markerfacecolor', 'r')\n",
    "plt.setp(sl, 'color', 'r')\n",
    "plt.setp(bl, visible=False)\n",
    "plt.xticks(ts)\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in on the low-frequency components, we can clearly see the yearly cycle dominating all other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.axis([0,3, 0, 5e13])\n",
    "plt.xlabel('Freq (1/yrs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing focus some more, we can cleary see the weekly cycle, at 52/year, as well as additional peaks at 12/year and 8/year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.axis([0,100, 0, 1e12])\n",
    "plt.xlabel('Freq (1/yrs)')\n",
    "plt.axvline(365/7., color='k', ls=':')\n",
    "plt.axvline(12, color='k', ls=':')\n",
    "plt.axvline(8, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detrending\n",
    "\n",
    "We wish to remove the seasonality we see in the data, so that we can better distinguish anomalous points.  To this end, we will build model of the ridership.  This is an example of **supervised machine learning**.\n",
    "\n",
    "In supervised machine learning, we have a $n \\times p$ **feature matrix** $X_{ji}$.  Each column corresponds to one of the $p$ features, and each row to a particular observation, out of $n$ total.  We also have a **label vector** $y_j$ of length $n$.  The goal is to develop a model $f$ that predicts the labels from the corresponding feature row; that is,\n",
    "\n",
    "$$ f(X_{j\\cdot}) \\approx y_j \\ . $$\n",
    "\n",
    "![Feature matrix](images/matrix.svg)\n",
    "\n",
    "When the labels are numerical, the problem is known as **regression**.  Then the labels represent different categories, the problem is one of **classification**.  Our problem, estimating counts, is a regression problem.\n",
    "\n",
    "Both regression and classification have a number of different metrics to gauge the effectiveness of a model.  The most commonly used metric for regression is the **mean-squared error** (MSE):\n",
    "\n",
    "$$ \\mbox{MSE} = \\frac 1 N \\sum_{j=1}^N \\left[ f(X_{j\\cdot}) - y_j \\right]^2 \\ . $$\n",
    "\n",
    "The MSE has units of $y^2$, which can make its size difficult to judge.  We often use the **root mean-squared error** (RMSE) instead.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "A basic, yet quite powerful, machine learning model is **linear regression**.  It is a linear model, meaning that\n",
    "\n",
    "$$ f(X_{j\\cdot}) = \\sum_i X_{ji} \\beta_i = (X \\cdot \\beta )_j \\ , $$\n",
    "\n",
    "for some $p$-vector $\\beta$.  Linear regression finds this vector by minimizing the MSE\n",
    "\n",
    "$$ \\frac 1 N \\left|X\\cdot\\beta - y\\right|^2 \\ . $$\n",
    "\n",
    "There is a closed-form solution:\n",
    "\n",
    "$$ \\hat\\beta = (X^T X)^{-1} X^T y \\ . $$\n",
    "\n",
    "Our first model will be to consider only the yearly cycles:\n",
    "\n",
    "$$ f(t) = A \\sin\\frac{2\\pi t}{365} + B\\cos\\frac{2\\pi t}{365} + f_0 \\ . $$\n",
    "\n",
    "At first glance, it may appear that linear regression is not suitable, since the model is not linear in $t$.  However, if we consider the $n\\times 2$ feature matrix\n",
    "\n",
    "$$ X = \\left[ \\sin\\frac{2\\pi t}{365}\\ \\ \\cos\\frac{2\\pi t}{365} \\right] \\ , $$\n",
    "\n",
    "we do actually have a linear model.\n",
    "\n",
    "### Scikit Learn\n",
    "\n",
    "This particular transformation is a particularly simple example of **feature engineering**.  Feature engineering is where much of the work of machine learning is done, so libraries will provide mechanisms to assist this process.\n",
    "\n",
    "We will be using the *Scikit Learn* module for machine learning.  It provides many tools, but the core is two types of classes: **transformers** and **estimators**.  Transformers take in a feature matrix and return a transformed version:\n",
    "``` python\n",
    "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    # Learn about the data\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    return ... # The transformed features\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FourierComponents(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dt = (X - self.X0).days * 2 * np.pi / self.period\n",
    "        return np.c_[np.sin(dt), np.cos(dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = FourierComponents(365)\n",
    "fc.fit(counts.index)\n",
    "plt.plot(fc.transform(counts.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators have a `predict()` method that returns the model's prediction for the label of a particular row.\n",
    "``` python                                                 \n",
    "class Estimator(base.BaseEstimator, base.RegressorMixin):\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    # Learn about the data\n",
    "    return self\n",
    "    \n",
    "  def predict(self, X):\n",
    "    return ... # The predicted labels\n",
    "```\n",
    "We'll use a linear regression estimator provided by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_trans = fc.transform(counts.index)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_trans, counts)\n",
    "plt.plot(counts.index, counts, counts.index, lr.predict(X_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling each transformer manually quickly gets tedious and error-prone.  Scikit Learn comes to the rescue with **pipelines**.  Pipelines take a series of transformers and (optionally) an estimator.  A pipeline acts as an estimator itself.  When it is fit, it fits the first transformer, transforms with the first transformer, uses that value to fit the second transformer, transforms with the second transformer, *etc.*, until finally fitting the estimator.  When predict is called on the pipeline, it sends the feature matrix through each of the transformers, before finally calling predict on the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('fourier', FourierComponents(365)),\n",
    "                 ('lr', LinearRegression())])\n",
    "pipe.fit(counts.index, counts)\n",
    "plt.plot(counts.index, counts, counts.index, pipe.predict(counts.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well are we doing?  A good baseline is the mean model, which has a MSE equal to the variance of the data.  We'll take the square root to look at RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(counts.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is definitely an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "We now consider the weekly cycle we saw.  If we group the results by day of the week, we can get a better feel for the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts.index[-1], counts.index[-1].dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame(\n",
    "    {'day': counts.index.dayofweek, 'count': counts.values}\n",
    ")\n",
    "day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_df.groupby('day').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "day_df.groupby('day').mean().plot(kind='bar')\n",
    "plt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekly cycle is not particularly sinusoidal.  Instead of treating the day of the week as a continuous variable, we will treat it as a **categorical feature**.  Such features denote membership in a class, without any particular ordering of those classes.  Therefore, we do not encode them in a single feature, but we create a new feature for each category.  Each row gets a 1 in the column corresponding to its category and a 0 in all others.  This is known as **one-hot encoding** or **dummy variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DayofWeek(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def day_vector(self, day):\n",
    "        v = np.zeros(7)\n",
    "        v[day] = 1\n",
    "        return v\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.stack(self.day_vector(d) for d in X.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DayofWeek().transform(counts.index)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used with linear regression, one-hot encoding produces per-category means.\n",
    "\n",
    "We want to send both the Fourier components and the one-hot encoded days to a linear regressor.  While we could write a transformer that does both, it's better to use a **feature union**.  This does in parallel what a pipeline does in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([('fourier', FourierComponents(365)),\n",
    "                      ('dayofweek', DayofWeek())])\n",
    "pipe = Pipeline([('union', union),\n",
    "                 ('lr', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe.fit(counts.index, counts)\n",
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what a model is doing correctly, and what it's missing, it's useful to plot the **residual**, the difference between the actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(counts - pipe.predict(counts.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridership has been growing with time.  The growth doesn't appear to be linear, but quadratic might be a good fit.  It's simple to add two more features representing $t$ and $t^2$ to attempt to fit this background growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuadBackground(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        days = (X - self.X0).days\n",
    "        return np.c_[days, days**2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([('date', QuadBackground()),\n",
    "                      ('fourier', FourierComponents(365)),\n",
    "                      ('dayofweek', DayofWeek())])\n",
    "pipe = Pipeline([('union', union),\n",
    "                 ('lr', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(counts.index, counts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(counts - pipe.predict(counts.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Account for the monthly seasonality.  Examine how ridership varies over the month.  Develop a model to account for this.  How much does this improve the RMSE?\n",
    "\n",
    "2. It seems reasonable to assume that weather affects the usage of the CitiBike system.  The `weatherdata/nycp.csv` file contains daily National Weather Service records for Central Park.  Add features from these records to your model.  Does this improve the RMSE of the model?  How much does ridership increase for every degree Fahrenheit?  (Hint: The coefficients of the linear model are stored in the `.coef_` attribute of a `LinearRegression` object.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
